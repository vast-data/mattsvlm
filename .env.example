# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL="gemma3:27b-it-qat"

# For remote Ollama server, uncomment and modify:
# OLLAMA_HOST=http://your-ollama-server:11434
# OLLAMA_MODEL=llava

# --- NEW: Endpoint Configuration ---
# Specify the service to use: "ollama" or "openai"
ENDPOINT_TYPE=ollama

# --- NEW: OpenAI Configuration (only needed if ENDPOINT_TYPE=openai) ---
OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
OPENAI_MODEL="gemma3:27b-it-qat" 
# Optional: Specify a custom base URL for OpenAI-compatible endpoints
# OPENAI_BASE_URL="http://your-proxy-or-local-endpoint:8000/v1"

# --- NEW: LLM Generation Parameters (apply to both endpoints) ---
# Controls randomness (0.0 = deterministic, 1.0 = more random). Typical values: 0.5 - 0.8
LLM_TEMPERATURE=0.7
# Controls nucleus sampling (1.0 = consider all likely tokens). Typical values: 0.9 - 1.0
LLM_TOP_P=1.0
